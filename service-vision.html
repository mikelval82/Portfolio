<!DOCTYPE html>
<html lang="es">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Computer Vision & Biosignals - Facial expression recognition and multimodal emotion detection">
  <meta name="author" content="Mikel Val">
  <title>Computer Vision & Biosignals | Mikel Val</title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="plugins/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="plugins/themify/css/themify-icons.css">
  <link rel="stylesheet" href="plugins/animate-css/animate.css">
  <link rel="stylesheet" href="plugins/aos/aos.css">
  <link rel="stylesheet" href="css/style.css">
</head>

<body class="portfolio" id="top">

<nav class="navbar navbar-expand-lg bg-transprent py-4 fixed-top navigation" id="navbar">
	<div class="container">
	  <a class="navbar-brand" href="index.html">
	  	<h2 class="logo">Mikel Val</h2>
	  </a>
	  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExample09" aria-controls="navbarsExample09" aria-expanded="false" aria-label="Toggle navigation">
		<span class="ti-view-list"></span>
	  </button>
  
	  <div class="collapse navbar-collapse text-center" id="navbarsExample09">
			<ul class="navbar-nav mx-auto">
			  <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#about">About</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#skillbar">Expertise</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#service">Services</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#contact">Contact</a></li>
			</ul>

		  	<ul class="list-inline mb-0 ml-lg-4 nav-social">
			  	<li class="list-inline-item"><a href="https://scholar.google.es/citations?user=PoviSYIAAAAJ&hl=es" target="_blank" title="Google Scholar"><i class="ti-book"></i></a></li>
			  	<li class="list-inline-item"><a href="https://www.linkedin.com/in/mikel-valencia-5138b1109/" target="_blank" title="LinkedIn"><i class="ti-linkedin"></i></a></li>
			  	<li class="list-inline-item"><a href="https://www.researchgate.net/profile/Mikel-Valencia" target="_blank" title="ResearchGate"><i class="ti-bookmark-alt"></i></a></li>
			  	<li class="list-inline-item"><a href="https://github.com/mikelval82" target="_blank" title="GitHub"><i class="ti-github"></i></a></li>
		  	</ul>
	  </div>
	</div>
</nav>

<section class="page-title">
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-lg-8">
          <div class="page-title text-center">
             <p>Computer Vision & AI</p>
              <h1>Computer Vision & Biosignals</h1>
          </div>
      </div>
    </div>
  </div>
</section>

<section class="section portfolio-single pt-0">
    <div class="container">
        <div class="row">
            <div class="col-lg-8">
                <div class="service-content">
                    <p class="lead mt-4 mb-5">Facial expression recognition, eye-tracking analysis, and multimodal emotion detection for advanced human-computer interaction.</p>

                    <div class="row">
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-face-smile mr-3 text-color"></i>Facial Recognition</h4>
                            <p>Facial recognition systems and emotional expression analysis using deep learning and specialised CNNs.</p>
                        </div>
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-eye mr-3 text-color"></i>Eye-Tracking</h4>
                            <p>Eye movement analysis and visual attention patterns for usability studies and neurofeedback.</p>
                        </div>
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-heart mr-3 text-color"></i>Biosignals Analysis</h4>
                            <p>Processing of biological signals (EEG, ECG, GSR) for detection of emotional and cognitive states.</p>
                        </div>
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-shine mr-3 text-color"></i>Multimodal Fusion</h4>
                            <p>Integration of multiple data sources (visual, auditory, biometric) for robust emotional analysis.</p>
                        </div>
                    </div>

                    <h3 class="mt-5 mb-4">Scientific Publications</h3>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">ðŸ“„ Affective Robot Story-telling Human-Robot Interaction</h5>
                        <p class="mb-2">Exploratory real-time emotion estimation analysis using facial expressions and physiological signals (EEG, BVP, GSR) in social robot interaction.</p>
                        <a href="https://www.researchgate.net/publication/342687554_Affective_Robot_Story-Telling_Human-Robot_Interaction_Exploratory_Real-Time_Emotion_Estimation_Analysis_Using_Facial_Expressions_and_Physiological_Signals" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication â†’
                        </a>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">ðŸ“„ Real-time Facial Expression Recognition using Smoothed Deep Neural Network Ensemble</h5>
                        <p class="mb-2">Real-time facial expression recognition system using CNN ensemble with label smoothing. State-of-the-art on FER2013, SFEW 2.0, and ExpW (13.48 ms on GPU).</p>
                        <a href="https://www.researchgate.net/publication/344141597_Real-time_facial_expression_recognition_using_smoothed_deep_neural_network_ensemble" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication â†’
                        </a>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">ðŸ“„ Real-time Emotional Recognition for Sociable Robotics Based on Deep Neural Networks Ensemble</h5>
                        <p class="mb-2">Emotional recognition system for social robots integrating YOLO facial detection and CNN ensemble for real-world interactions. IWINAC 2019.</p>
                        <a href="https://www.researchgate.net/publication/332966267_Real-Time_Emotional_Recognition_for_Sociable_Robotics_Based_on_Deep_Neural_Networks_Ensemble" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication â†’
                        </a>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">ðŸ“„ Horizon Cyber-vision: A Cybernetic Approach for a Cortical Visual Prosthesis</h5>
                        <p class="mb-2">Configurable wearable system for cortical visual prosthesis using Deep Learning (object detection, monocular depth estimation, edge detection) and eye-tracking.</p>
                        <a href="https://dialnet.unirioja.es/servlet/articulo?codigo=8733712" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication â†’
                        </a>
                    </div>

                    <h3 class="mt-5 mb-4">Featured Projects</h3>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-3">ðŸ§  ELITE - Competency Analysis and Prediction System</h5>
                        <p class="mb-2">Comprehensive system for analysing and predicting cognitive, socioemotional, and leadership competencies through multimodal data integration combining eye-tracking, facial expression recognition, audio analysis, and neuropsychological assessment.</p>
                        <ul class="list-unstyled">
                            <li><strong>Module 1 (Cognitive):</strong> Processing speed, attention (divided/selective), creative thinking, self-confidence, emotional self-regulation through eye-tracking and temporal markers</li>
                            <li><strong>Module 2 (Socioemotional):</strong> Verbal/non-verbal/paraverbal communication, empathy, assertiveness, active listening through facial expression analysis and audio features</li>
                            <li><strong>Module 3 (Neuropsychological):</strong> Impulsivity (BIS), cognitive flexibility (WISC), planning (TOWERS), attention (D2), reasoning (MAT), creativity (KEY), working memory (DIGIT)</li>
                            <li><strong>Module 4 (Audio/Psychometric):</strong> Assertiveness (RATHUS), transformational leadership (MLQ), empathy (TECA) through advanced speech signal processing</li>
                            <li><strong>Technologies:</strong> Python 3.12, PyTorch, scikit-learn, XGBoost, librosa for audio processing, real-time eye-tracking analysis</li>
                            <li><strong>Machine Learning:</strong> Ensemble models with feature extraction pipelines, trained classification models achieving multi-class prediction (HIGH/MEDIUM/LOW competency levels)</li>
                            <li><strong>Deployment:</strong> Standalone executable via PyInstaller for cross-platform distribution, modular architecture with independent competency evaluators</li>
                            <li><strong>Innovation:</strong> Multimodal fusion framework integrating visual (eye-tracking), audio (prosody, acoustic features), and behavioural data for holistic competency assessment</li>
                            <li><strong>Output:</strong> Structured JSON reports with competency classifications based on validated psychometric scales and neuropsychological tests</li>
                        </ul>
                    </div>

                    <h3 class="mt-5 mb-4">Technologies and Tools</h3>
                    <div class="row">
                        <div class="col-lg-6">
                            <ul class="lead">
                                <li><strong>Deep Learning:</strong> PyTorch, TensorFlow, Keras</li>
                                <li><strong>Computer Vision:</strong> OpenCV, MediaPipe, Dlib</li>
                                <li><strong>Architectures:</strong> ResNet, VGG, EfficientNet, YOLO</li>
                            </ul>
                        </div>
                        <div class="col-lg-6">
                            <ul class="lead">
                                <li><strong>Biosignals:</strong> BioSPPy, MNE-Python, NeuroKit</li>
                                <li><strong>Hardware:</strong> Tobii Eye Tracker, EmotiBit, RealSense</li>
                                <li><strong>Datasets:</strong> FER2013, AffectNet, CK+, CASME</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <div class="col-lg-4">
                <div class="portfolio-sidebar mt-5 mt-lg-0">
                    <div class="card bg-gray p-4">
                        <h4 class="card-title text-center mb-4 pt-3">Service Information</h4>

                        <ul class="list-unstyled">
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Specialisation:</strong>
                                <span>Computer Vision</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Experience:</strong>
                                <span>8+ years</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Publications:</strong>
                                <span>15+ papers</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Frameworks:</strong>
                                <span>PyTorch, OpenCV</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Modalities:</strong>
                                <span>Visual, Audio, Bio</span>
                            </li>
                            <li class="text-center mt-4">
                               <a href="index.html#contact" class="btn btn-main">Contact</a>
                            </li>
                        </ul>
                    </div>

                    <div class="card bg-gray p-4 mt-4">
                        <h4 class="text-center mb-4">Have a project in mind?</h4>
                         <a href="index.html#contact" class="btn btn-solid-border">Let's Talk</a>
                    </div>

                    <div class="card bg-gray p-4 mt-4">
                        <h5 class="mb-3">Related Services</h5>
                        <ul class="list-unstyled">
                            <li class="mb-2"><a href="service-llm.html">â†’ LLM-Powered Solutions</a></li>
                            <li class="mb-2"><a href="service-health.html">â†’ Digital Health & AI</a></li>
                            <li class="mb-2"><a href="service-neuroprosthetics.html">â†’ Neuroprosthetics & Robotics</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<footer class="footer border-top-1">
	<div class="container">
		<div class="row align-items-center text-center text-lg-left">
			<div class="col-lg-3">
				<h3 class="logo mb-4">Mikel Val</h3>
			</div>
			<div class="col-lg-5">
				<ul class="list-inline footer-socials">
					<li class="list-inline-item"><a href="https://scholar.google.es/citations?user=PoviSYIAAAAJ&hl=es" target="_blank" title="Google Scholar"><i class="ti-book"></i></a></li>
					<li class="list-inline-item"><a href="https://www.linkedin.com/in/mikel-valencia-5138b1109/" target="_blank" title="LinkedIn"><i class="ti-linkedin"></i></a></li>
					<li class="list-inline-item"><a href="https://www.researchgate.net/profile/Mikel-Valencia" target="_blank" title="ResearchGate"><i class="ti-bookmark-alt"></i></a></li>
					<li class="list-inline-item"><a href="https://github.com/mikelval82" target="_blank" title="GitHub"><i class="ti-github"></i></a></li>
				</ul>
			</div>
			<div class="col-lg-4">
				<p class="lead">&copy; 2025 Mikel Val. Senior Researcher at HUMAN-TECH Research Center</p>
				<a href="#top" class="backtop smoth-scroll"><i class="ti-angle-up"></i></a>
			</div>
		</div>
	</div>
</footer>

    <script src="plugins/jquery/jquery.min.js"></script>
    <script src="plugins/bootstrap/js/popper.js"></script>
    <script src="plugins/bootstrap/js/bootstrap.min.js"></script>
    <script src="plugins/aos/aos.js"></script>
    <script src="js/script.js"></script>

  </body>
</html>
