<!DOCTYPE html>
<html lang="es">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Computer Vision & Biosignals - Facial expression recognition and multimodal emotion detection">
  <meta name="author" content="Mikel Val">
  <title>Computer Vision & Biosignals | Mikel Val</title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="plugins/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="plugins/themify/css/themify-icons.css">
  <link rel="stylesheet" href="plugins/animate-css/animate.css">
  <link rel="stylesheet" href="plugins/aos/aos.css">
  <link rel="stylesheet" href="css/style.css">
</head>

<body class="portfolio" id="top">

<nav class="navbar navbar-expand-lg bg-transprent py-4 fixed-top navigation" id="navbar">
	<div class="container">
	  <a class="navbar-brand" href="index.html">
	  	<h2 class="logo">Mikel Val</h2>
	  </a>
	  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExample09" aria-controls="navbarsExample09" aria-expanded="false" aria-label="Toggle navigation">
		<span class="ti-view-list"></span>
	  </button>
  
	  <div class="collapse navbar-collapse text-center" id="navbarsExample09">
			<ul class="navbar-nav mx-auto">
			  <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#about">About</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#skillbar">Expertise</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#service">Services</a></li>
			   <li class="nav-item"><a class="nav-link" href="index.html#contact">Contact</a></li>
			</ul>

		  	<ul class="list-inline mb-0 ml-lg-4 nav-social">
			  	<li class="list-inline-item"><a href="https://scholar.google.es/citations?user=PoviSYIAAAAJ&hl=es" target="_blank" title="Google Scholar"><i class="ti-book"></i></a></li>
			  	<li class="list-inline-item"><a href="https://www.linkedin.com/in/mikel-valencia-5138b1109/" target="_blank" title="LinkedIn"><i class="ti-linkedin"></i></a></li>
			  	<li class="list-inline-item"><a href="https://www.researchgate.net/profile/Mikel-Valencia" target="_blank" title="ResearchGate"><i class="ti-bookmark-alt"></i></a></li>
			  	<li class="list-inline-item"><a href="https://github.com/mikelval82" target="_blank" title="GitHub"><i class="ti-github"></i></a></li>
		  	</ul>
	  </div>
	</div>
</nav>

<section class="page-title">
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-lg-8">
          <div class="page-title text-center">
             <p>Computer Vision & AI</p>
              <h1>Computer Vision & Biosignals</h1>
          </div>
      </div>
    </div>
  </div>
</section>

<section class="section portfolio-single pt-0">
    <div class="container">
        <div class="row">
            <div class="col-lg-8">
                <div class="service-content">
                    <p class="lead mt-4 mb-5">Facial expression recognition, eye-tracking analysis, and multimodal emotion detection for advanced human-computer interaction.</p>

                    <div class="row">
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-face-smile mr-3 text-color"></i>Facial Recognition</h4>
                            <p>Facial recognition systems and emotional expression analysis using deep learning and specialised CNNs.</p>
                        </div>
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-eye mr-3 text-color"></i>Eye-Tracking</h4>
                            <p>Eye movement analysis and visual attention patterns for usability studies and neurofeedback.</p>
                        </div>
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-heart mr-3 text-color"></i>Biosignals Analysis</h4>
                            <p>Processing of biological signals (EEG, ECG, GSR) for detection of emotional and cognitive states.</p>
                        </div>
                        <div class="col-lg-6 mb-4">
                            <h4 class="mb-3"><i class="ti-shine mr-3 text-color"></i>Multimodal Fusion</h4>
                            <p>Integration of multiple data sources (visual, auditory, biometric) for robust emotional analysis.</p>
                        </div>
                    </div>

                    <h3 class="mt-5 mb-4">Featured Projects</h3>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-3">üß† ELITE - Competency Analysis and Prediction System</h5>
                        <p class="mb-2">Comprehensive system for analysing and predicting cognitive, socioemotional, and leadership competencies through multimodal data integration combining eye-tracking, facial expression recognition, audio analysis, and neuropsychological assessment. Evidence-based platform enabling objective assessment of leadership style and ability through independent virtual environments and serious games with implicit measures.</p>
                        <ul class="list-unstyled">
                            <li><strong>Grant:</strong> CPP2021-008569 funded by MICIU/AEI/10.13039/501100011033 and European Union NextGenerationEU/PRTR</li>
                            <li><strong>Approach:</strong> Practical, realistic, evidence-based methodology for objective competency evaluation through hidden assessment (implicit measures) in serious games within virtual environments</li>
                            <li><strong>Module 1 (Cognitive):</strong> Processing speed, attention (divided/selective), creative thinking, self-confidence, emotional self-regulation through eye-tracking and temporal markers</li>
                            <li><strong>Module 2 (Socioemotional):</strong> Verbal/non-verbal/paraverbal communication, empathy, assertiveness, active listening through facial expression analysis and audio features</li>
                            <li><strong>Module 3 (Neuropsychological):</strong> Impulsivity (BIS), cognitive flexibility (WISC), planning (TOWERS), attention (D2), reasoning (MAT), creativity (KEY), working memory (DIGIT)</li>
                            <li><strong>Module 4 (Audio/Psychometric):</strong> Assertiveness (RATHUS), transformational leadership (MLQ), empathy (TECA) through advanced speech signal processing</li>
                            <li><strong>Technologies:</strong> Python 3.12, PyTorch, scikit-learn, XGBoost, librosa for audio processing, real-time eye-tracking analysis, independent virtual environments per competency</li>
                            <li><strong>Machine Learning:</strong> Ensemble models with feature extraction pipelines, trained classification models achieving multi-class prediction (HIGH/MEDIUM/LOW competency levels), clustering algorithms for behaviour prediction</li>
                            <li><strong>Deployment:</strong> Standalone executable via PyInstaller for cross-platform distribution, modular architecture with independent competency evaluators</li>
                            <li><strong>Innovation:</strong> Multimodal fusion framework integrating visual (eye-tracking), audio (prosody, acoustic features), and behavioural data for holistic competency assessment through serious games</li>
                            <li><strong>Output:</strong> Automated performance reports and structured JSON classifications based on validated psychometric scales and neuropsychological tests, behaviour prediction algorithms</li>
                        </ul>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-3">üí∂ NeuroCash - Multimodal Analysis of Banknote Design Perception</h5>
                        <p class="mb-2">Academic research project analysing cognitive and emotional responses to banknote designs through multimodal data integration. Commissioned by Banco de Espa√±a, the system combines eye-tracking, gesture recognition, facial coding, speech analysis, and psychometric questionnaires to understand user perception and interaction with currency aesthetics. <strong>Winner of UPV Excellence Award for Knowledge Transfer 2025.</strong></p>
                        <ul class="list-unstyled">
                            <li><strong>Multimodal Data Sources:</strong> Eye-tracking (heatmaps, scanpaths, attention metrics), gesture recognition (flip, tilt, rotate, rub), facial expression analysis (emotion detection: happiness, anger, sadness), speech transcription and sentiment analysis, standardised questionnaire responses</li>
                            <li><strong>Research Questions:</strong> Impact of demographics (age, gender, education, cash usage) on gaze patterns and gestures, correlation between design ratings and behavioural responses, emotional triggers by different banknote designs, hypothesis testing (e.g., rotation behaviour for vertical vs. non-vertical designs)</li>
                            <li><strong>Technologies:</strong> Python 3.x, eye-tracking hardware integration, facial coding libraries, NLP modules for sentiment extraction, statistical analysis frameworks</li>
                            <li><strong>Segmentation Analysis:</strong> Demographic-based segmentation (gender, age groups, education levels, cash usage frequency), rating-based grouping (high vs. low scores), gesture frequency and type analysis per banknote design</li>
                            <li><strong>Visualisation Pipeline:</strong> Customisable plotting functions for gesture distributions, facial expression timelines, attention heatmaps, survey result comparisons, statistical significance testing</li>
                            <li><strong>Data Processing:</strong> Modular scripts for gesture loading, heatmap analysis, facial coding processing, speech transcription, questionnaire data integration</li>
                            <li><strong>Innovation:</strong> Holistic approach combining behavioural (gestures), physiological (gaze, facial expressions), linguistic (speech sentiment), and self-report data (questionnaires) for comprehensive currency design evaluation</li>
                            <li><strong>Impact:</strong> Provides evidence-based insights for Banco de Espa√±a on banknote design optimisation, accessibility, and user experience across diverse demographics</li>
                        </ul>
                    </div>

                    <div class="card bg-success text-white p-4 mb-4">
                        <h5 class="mb-3 text-white">üèÜ NeuroCash - UPV Excellence in Knowledge Transfer Award 2025</h5>
                        <p class="mb-2">Pioneering collaboration project between LabLENI - Instituto HumanTech UPV and Banco de Espa√±a to investigate perception, emotion, and trust generated by banknotes through neuroscience, design, and behavioural economics.</p>
                        <ul class="list-unstyled text-white">
                            <li><strong>Recognition:</strong> UPV Excellence in Knowledge Transfer Award 2025 (Category: Arts and Humanities, Social Sciences and Architecture)</li>
                            <li><strong>Collaboration:</strong> Universitat Polit√®cnica de Val√®ncia & Banco de Espa√±a (8 years of research)</li>
                            <li><strong>Key questions:</strong> How do people perceive money? What emotions and trust do banknote design and security features evoke?</li>
                            <li><strong>Results:</strong> 2 patents, numerous scientific publications, collaborations with national and international financial institutions</li>
                            <li><strong>Techniques:</strong> Applied neuroscience, eye-tracking, EEG, emotional response analysis, user-centred design</li>
                            <li><strong>LabLENI-HumanTech Team:</strong> Mariano Alca√±iz Raya, Jaime Guixeres, Fernando L√≥pez Mir, Elena Letrado, <strong>Mikel Val Calvo</strong>, Javier Mar√≠n, Carmen Calero, V√≠ctor Gasc√≥, Adri√°n Colomer, Valery Naranjo</li>
                            <li class="mt-3"><a href="https://www.linkedin.com/posts/instituto-human-tech-upv_lableni-humantech-upv-activity-7391837760145281024-bBze?utm_source=share&utm_medium=member_desktop" target="_blank" class="btn btn-light btn-sm"><i class="ti-linkedin mr-2"></i>View full news</a></li>
                        </ul>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-3">üéØ PICTURE - VR-based Platform for Leadership Assessment (CPP2021-008517, 2022-2025)</h5>
                        <p class="mb-2"><strong>Public-Private Partnership:</strong> LabLENI (UPV) + HGBS CONSOURCING, S.L | <strong>Funded by:</strong> MICIU/AEI/10.13039/501100011033 + European Union NextGenerationEU/PRTR</p>
                        <p class="mb-2">VR-based comprehensive assessment platform utilising multimodal machine learning to evaluate horizontal competencies (decision-making, interpersonal abilities) through implicit measures in hyper-realistic virtual environments. The system captures eye-tracking, facial expressions, voice analysis, psychophysiological signals, and behavioural data to provide objective leadership and cognitive-emotional profiling.</p>
                        <ul class="list-unstyled">
                            <li><strong>VR Assessment Context:</strong> Hyper-realistic virtual scenarios for evaluating horizontal competencies: critical thinking, decision-making, interpersonal communication, emotional regulation, assertiveness, empathy, strategic planning</li>
                            <li><strong>Implicit Measures Integration:</strong> Eye-tracking metrics (AOI visits, fixation patterns, gaze transitions), voice features (pitch, intensity, jitter, shimmer, HNR, speech rate), facial expression analysis, psychophysiological signals, behavioural interaction logs</li>
                            <li><strong>VR Activities:</strong> Character selection & CV/video interactions, ZOO spatial planning puzzle (cognitive performance), assertiveness negotiation tasks, presentation scenarios, leadership interview simulations with Big Five personality assessment (N/E/O/A/C)</li>
                            <li><strong>Competencies Evaluated:</strong> Critical thinking (F1: 0.640, 80% accuracy), strategic planning, attachment style (embeddings-based), assertiveness, emotional self-regulation, empathy (F1: 0.739, 74% accuracy via presentation ET), Big Five personality traits</li>
                            <li><strong>Machine Learning Pipeline:</strong> RandomForestClassifier, XGBoost, GradientBoosting, AdaBoost, SVC, LogisticRegression, custom neural networks for embedding-based classification, trait-specific model optimization</li>
                            <li><strong>Technologies:</strong> Unity VR platform, Python 3.12, PyTorch, scikit-learn, librosa (audio analysis), RoBERTa (NLP embeddings), Tobii eye-tracking integration, modular feature extraction pipeline</li>
                            <li><strong>Advanced Features:</strong> Cosine similarity loss for attachment embeddings, automatic category mapping and normalisation, multimodal fusion (gaze + voice + behaviour), fine-tuned sentence transformers for verbal interaction analysis</li>
                            <li><strong>Data Architecture:</strong> Standardised user data structure (Activities/, EyeTrackingData/), automated transcript processing, feature consolidation across modalities, model versioning with .pkl serialisation</li>
                            <li><strong>Performance Highlights:</strong> Critical thinking (80% accuracy), empathy via presentation eye-tracking (74% accuracy), attachment style via NLP embeddings (62% accuracy), Big Five personality (38-55% accuracy, 3-class problem)</li>
                            <li><strong>Innovation:</strong> First integrated VR platform combining implicit measures (physiological, gaze, voice) with behavioural and linguistic features for objective leadership competency assessment, eliminating self-report bias through hyper-realistic immersive context</li>
                        </ul>
                    </div>

                    <h3 class="mt-5 mb-4">Scientific Publications</h3>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">üìÑ Affective Robot Story-telling Human-Robot Interaction</h5>
                        <p class="mb-2">Exploratory real-time emotion estimation analysis using facial expressions and physiological signals (EEG, BVP, GSR) in social robot interaction.</p>
                        <a href="https://www.researchgate.net/publication/342687554_Affective_Robot_Story-Telling_Human-Robot_Interaction_Exploratory_Real-Time_Emotion_Estimation_Analysis_Using_Facial_Expressions_and_Physiological_Signals" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication ‚Üí
                        </a>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">üìÑ Real-time Facial Expression Recognition using Smoothed Deep Neural Network Ensemble</h5>
                        <p class="mb-2">Real-time facial expression recognition system using CNN ensemble with label smoothing. State-of-the-art on FER2013, SFEW 2.0, and ExpW (13.48 ms on GPU).</p>
                        <a href="https://www.researchgate.net/publication/344141597_Real-time_facial_expression_recognition_using_smoothed_deep_neural_network_ensemble" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication ‚Üí
                        </a>
                    </div>

                    <div class="card bg-gray p-4 mb-4">
                        <h5 class="mb-2">üìÑ Real-time Emotional Recognition for Sociable Robotics Based on Deep Neural Networks Ensemble</h5>
                        <p class="mb-2">Emotional recognition system for social robots integrating YOLO facial detection and CNN ensemble for real-world interactions. IWINAC 2019.</p>
                        <a href="https://www.researchgate.net/publication/332966267_Real-Time_Emotional_Recognition_for_Sociable_Robotics_Based_on_Deep_Neural_Networks_Ensemble" target="_blank" class="text-color">
                            <i class="ti-link mr-2"></i>View publication ‚Üí
                        </a>
                    </div>

                    <h3 class="mt-5 mb-4">Technologies and Tools</h3>
                    <div class="row">
                        <div class="col-lg-6">
                            <ul class="lead">
                                <li><strong>Deep Learning:</strong> PyTorch, TensorFlow, Keras</li>
                                <li><strong>Computer Vision:</strong> OpenCV, MediaPipe, Dlib</li>
                                <li><strong>Architectures:</strong> ResNet, VGG, EfficientNet, YOLO</li>
                            </ul>
                        </div>
                        <div class="col-lg-6">
                            <ul class="lead">
                                <li><strong>Biosignals:</strong> BioSPPy, MNE-Python, NeuroKit</li>
                                <li><strong>Hardware:</strong> Tobii Eye Tracker, EmotiBit, RealSense</li>
                                <li><strong>Datasets:</strong> FER2013, AffectNet, CK+, CASME</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <div class="col-lg-4">
                <div class="portfolio-sidebar mt-5 mt-lg-0">
                    <div class="card bg-gray p-4">
                        <h4 class="card-title text-center mb-4 pt-3">Service Information</h4>

                        <ul class="list-unstyled">
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Specialisation:</strong>
                                <span>Computer Vision</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Experience:</strong>
                                <span>8+ years</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Publications:</strong>
                                <span>15+ papers</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Frameworks:</strong>
                                <span>PyTorch, OpenCV</span>
                            </li>
                            <li class="d-flex justify-content-between align-content-center mb-3">
                                <strong>Modalities:</strong>
                                <span>Visual, Audio, Bio</span>
                            </li>
                            <li class="text-center mt-4">
                               <a href="index.html#contact" class="btn btn-main">Contact</a>
                            </li>
                        </ul>
                    </div>

                    <div class="card bg-gray p-4 mt-4">
                        <h4 class="text-center mb-4">Have a project in mind?</h4>
                         <a href="index.html#contact" class="btn btn-solid-border">Let's Talk</a>
                    </div>

                    <div class="card bg-gray p-4 mt-4">
                        <h5 class="mb-3">Related Services</h5>
                        <ul class="list-unstyled">
                            <li class="mb-2"><a href="service-llm.html">‚Üí LLM-Powered Solutions</a></li>
                            <li class="mb-2"><a href="service-health.html">‚Üí Digital Health & AI</a></li>
                            <li class="mb-2"><a href="service-neuroprosthetics.html">‚Üí Neuroprosthetics & Robotics</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<footer class="footer border-top-1">
	<div class="container">
		<div class="row align-items-center text-center text-lg-left">
			<div class="col-lg-3">
				<h3 class="logo mb-4">Mikel Val</h3>
			</div>
			<div class="col-lg-5">
				<ul class="list-inline footer-socials">
					<li class="list-inline-item"><a href="https://scholar.google.es/citations?user=PoviSYIAAAAJ&hl=es" target="_blank" title="Google Scholar"><i class="ti-book"></i></a></li>
					<li class="list-inline-item"><a href="https://www.linkedin.com/in/mikel-valencia-5138b1109/" target="_blank" title="LinkedIn"><i class="ti-linkedin"></i></a></li>
					<li class="list-inline-item"><a href="https://www.researchgate.net/profile/Mikel-Valencia" target="_blank" title="ResearchGate"><i class="ti-bookmark-alt"></i></a></li>
					<li class="list-inline-item"><a href="https://github.com/mikelval82" target="_blank" title="GitHub"><i class="ti-github"></i></a></li>
				</ul>
			</div>
			<div class="col-lg-4">
				<p class="lead">&copy; 2025 Mikel Val. Senior Researcher at HUMAN-TECH Research Center</p>
				<a href="#top" class="backtop smoth-scroll"><i class="ti-angle-up"></i></a>
			</div>
		</div>
	</div>
</footer>

    <script src="plugins/jquery/jquery.min.js"></script>
    <script src="plugins/bootstrap/js/popper.js"></script>
    <script src="plugins/bootstrap/js/bootstrap.min.js"></script>
    <script src="plugins/aos/aos.js"></script>
    <script src="js/script.js"></script>

  </body>
</html>
